F_thresh <- qf(0.5,num_df,den_df)
influential <- cook_dist[which(cook_dist > F_thresh)]
influential
# Investigate
View(t(clean_data[78,]))
View(t(clean_data[64,]))
plot(aic, which = 5)
cook_dist[which(cook_dist > 0.5)]
bc <- MASS::boxcox(aic, plotit = T)
lambda <- bc$x[which.max(bc$y)]
mod_bc <- lm(ViolentCrimesPerPop^lambda ~ population + householdsize + racepctblack + racePctWhite + racePctHisp +
agePct12t21 + agePct12t29 + agePct16t24 + agePct65up + pctUrban +
pctWWage + pctWInvInc + pctWSocSec + perCapInc + whitePerCap +
blackPerCap + indianPerCap + AsianPerCap + OtherPerCap +
HispPerCap + PctPopUnderPov + PctLess9thGrade + PctNotHSGrad +
PctBSorMore + PctUnemployed + PctEmploy + PctEmplManu + PctEmplProfServ +
PctOccupManu + PctOccupMgmtProf + MalePctDivorce + MalePctNevMarr +
FemalePctDiv + PersPerFam + PctFam2Par + PctTeen2Par + PctWorkMomYoungKids +
PctWorkMom + PctIlleg + NumImmig + PctImmigRec5 + PctImmigRec8 +
PctImmigRec10 + PctRecentImmig + PctRecImmig5 + PctRecImmig8 +
PctRecImmig10 + PctSpeakEnglOnly + PctNotSpeakEnglWell +
PctLargHouseFam + PctLargHouseOccup + PersPerOccupHous +
PersPerOwnOccHous + PersPerRentOccHous + PctPersOwnOccup +
PctPersDenseHous + PctHousLess3BR + MedNumBR + HousVacant +
PctHousOccup + PctHousOwnOcc + PctVacantBoarded + PctVacMore6Mos +
PctHousNoPhone + PctWOFullPlumb + OwnOccLowQuart + OwnOccMedVal +
OwnOccHiQuart + RentLowQ + RentMedian + RentHighQ + MedRent +
MedRentPctHousInc + MedOwnCostPctInc + MedOwnCostPctIncNoMtg +
NumInShelters + NumStreet + PctForeignBorn + PctBornSameState +
PctSameHouse85 + PctSameCity85 + PctSameState85 + LemasSwornFT +
LemasSwFTPerPop + LemasSwFTFieldPerPop + LemasTotReqPerPop +
PolicReqPerOffic + PolicPerPop + RacialMatchCommPol + PctPolicWhite +
PctPolicBlack + PctPolicHisp + PctPolicMinor + OfficAssgnDrugUnits +
NumKindsDrugsSeiz + LandArea + PopDens + PctUsePubTrans +
PolicCars + LemasPctPolicOnPatr + LemasPctOfficDrugUn + PolicBudgPerPop, clean_data)
bptest(mod_bc)
p1 <- plot(aic$fitted.values,aic$residuals)
p2 <- plot(mod_bc$fitted.values,mod_bc$residuals)
gridExtra::grid.arrange(p1, p2)
plot((p1, p2))
plot(p1, p2)
par(mfrow=c(2,2))
p1
bptest(mod_bc)
p1 <- plot(aic$fitted.values,aic$residuals)
p2 <- plot(mod_bc$fitted.values,mod_bc$residuals)
par(mfrow=c(2,2))
p1
p2
bptest(mod_bc)
par(mfrow=c(2,2))
plot(aic$fitted.values,aic$residuals)
plot(mod_bc$fitted.values,mod_bc$residuals)
par(mfrow=c(2,2))
plot(aic$fitted.values,aic$residuals)
plot(mod_bc$fitted.values,mod_bc$residuals)
par(mfrow=c(1,2))
plot(aic$fitted.values,aic$residuals)
plot(mod_bc$fitted.values,mod_bc$residuals)
bptest(mod_bc)
par(mfrow=c(1,2))
plot(aic$fitted.values,aic$residuals)
plot(mod_bc$fitted.values,mod_bc$residuals)
par(mfrow=c(1,2))
plot(aic$fitted.values,aic$residuals)
plot(mod_bc$fitted.values,mod_bc$residuals)
MASS::boxcox(aic, plotit = T)
?MASS::boxcox
bc <- MASS::boxcox(aic, lambda = seq(0, 1, 0.001), plotit = T)
lambda <- bc$x[which.max(bc$y)]
MASS::boxcox(aic, lambda = seq(0, 1, 0.001), plotit = T)
bptest(aic)
library(tidyverse)
library(randomForest)
library(mosaic)
profiles <- okcupiddata::profiles
cats <- c("dislikes cats", "has cats", "likes cats")
positive <- c("likes dogs and dislikes cats", "likes dogs and has cats", "likes dogs and likes cats", "likes dogs")
neutral <- c("has dogs and has cats", "has dogs and likes cats", "has dogs", "has dogs and dislikes cats")
negative <- c("dislikes dogs and likes cats", "dislikes dogs", "dislikes dogs and dislikes cats", "dislikes dogs and has cats")
# Remove NA's and info about cats
data <- filter(profiles, !(pets %in% cats))
data <- select(data, -essay0)
# Create a 1-0 indicator variable for the likes_dogs
data$likes_dogs <- ifelse(data$pets %in% positive, 1, 0)
data$likes_dogs <- as.factor(data$likes_dogs)
glimpse(data %>% mutate_if(is.character, as.factor))
type(data$body_type)
typeof(data$body_type)
# Create factors
data <- data %>% mutate_if(is.character, as.factor)
# Set seed to 1847
set.seed(1847)
# Divide the data randomly with 80% of the data in the training set and 20% in the test set
n <- nrow(data)
test_idx <- sample.int(n, size = round(0.2 * n))
train <- data[-test_idx, ]
test <- data[test_idx, ]
form <- as.formula(likes_dogs~.)
random_forest<- randomForest(form, data = train, mtry=10, ntree=100,na.action=na.roughfix)
library(tidyverse)
library(randomForest)
library(mosaic)
profiles <- okcupiddata::profiles
cats <- c("dislikes cats", "has cats", "likes cats")
positive <- c("likes dogs and dislikes cats", "likes dogs and has cats", "likes dogs and likes cats", "likes dogs")
neutral <- c("has dogs and has cats", "has dogs and likes cats", "has dogs", "has dogs and dislikes cats")
negative <- c("dislikes dogs and likes cats", "dislikes dogs", "dislikes dogs and dislikes cats", "dislikes dogs and has cats")
# Remove NA's and info about cats
data <- filter(profiles, !(pets %in% cats))
data <- select(data, -essay0)
# Create a 1-0 indicator variable for the likes_dogs
data$likes_dogs <- ifelse(data$pets %in% positive, 1, 0)
data$likes_dogs <- as.factor(data$likes_dogs)
# Create factors
data <- data %>% mutate_if(is.character, as.factor)
# Set seed to 1847
set.seed(1847)
# Divide the data randomly with 80% of the data in the training set and 20% in the test set
n <- nrow(data)
test_idx <- sample.int(n, size = round(0.2 * n))
train <- data[-test_idx, ]
test <- data[test_idx, ]
form <- as.formula(likes_dogs~.)
random_forest<- randomForest(form, data = train, mtry=10, ntree=100,na.action=na.roughfix)
na.roughfix(data)
na.roughfix(data$likes_dogs)
na.roughfix(data$age)
na.roughfix(data$2)
na.roughfix(data[3])
na.roughfix(data[4])
na.roughfix(data[5])
na.roughfix(data[6])
na.roughfix(data[7])
na.roughfix(data[8])
na.roughfix(data[9])
na.roughfix(data[10])
na.roughfix(data[11])
data$last_online
na.roughfix()data$last_online)
na.roughfix(data$last_online)
na.roughfix(data[11])
data[11]
names(data[11])
na.roughfix(data$last_online)
data <- data %>%
mutate(time_elapsed_login = interval(last_online, max(profiles$last_online)) / dhours(1))
data <- data %>%
mutate(time_elapsed_login = lubridate::interval(last_online, max(profiles$last_online)) / dhours(1))
library(lubridate)
data <- data %>%
mutate(time_elapsed_login = interval(last_online, max(profiles$last_online)) / dhours(1))
cats <- c("dislikes cats", "has cats", "likes cats")
positive <- c("likes dogs and dislikes cats", "likes dogs and has cats", "likes dogs and likes cats", "likes dogs")
neutral <- c("has dogs and has cats", "has dogs and likes cats", "has dogs", "has dogs and dislikes cats")
negative <- c("dislikes dogs and likes cats", "dislikes dogs", "dislikes dogs and dislikes cats", "dislikes dogs and has cats")
# Remove NA's and info about cats
data <- filter(profiles, !(pets %in% cats), !is.na(pets))
# Create a 1-0 indicator variable for the likes_dogs
data$likes_dogs <- ifelse(data$pets %in% positive, 1, 0)
data$likes_dogs <- as.factor(data$likes_dogs)
# Create factors
data <- data %>% mutate_if(is.character, as.factor)
data <- data %>%
mutate(time_elapsed_login = interval(last_online, max(profiles$last_online)) / dhours(1))
data <- select(data, -essay0, -last_online)
# Set seed to 1847
set.seed(1847)
# Divide the data randomly with 80% of the data in the training set and 20% in the test set
n <- nrow(data)
test_idx <- sample.int(n, size = round(0.2 * n))
train <- data[-test_idx, ]
test <- data[test_idx, ]
form <- as.formula(likes_dogs~.)
random_forest<- randomForest(form, data = train, mtry=10, ntree=100,na.action=na.roughfix)
pca <- prcomp(train, scale = TRUE)
```{r}
form <- as.formula(likes_dogs~.)
random_forest<- randomForest(form, data = train, mtry=10, ntree=100,na.action=na.roughfix)
map_df(data, levels)
map_df(data, levels())
map_df(data, levels
)
typeof(data)
typeof(profiles)
profiles <- as.data.frame(okcupiddata::profiles)
cats <- c("dislikes cats", "has cats", "likes cats")
positive <- c("likes dogs and dislikes cats", "likes dogs and has cats", "likes dogs and likes cats", "likes dogs")
neutral <- c("has dogs and has cats", "has dogs and likes cats", "has dogs", "has dogs and dislikes cats")
negative <- c("dislikes dogs and likes cats", "dislikes dogs", "dislikes dogs and dislikes cats", "dislikes dogs and has cats")
# Remove NA's and info about cats
data <- filter(profiles, !(pets %in% cats), !is.na(pets))
# Create a 1-0 indicator variable for the likes_dogs
data$likes_dogs <- ifelse(data$pets %in% positive, 1, 0)
data$likes_dogs <- as.factor(data$likes_dogs)
# Create factors
data <- data %>% mutate_if(is.character, as.factor)
data <- data %>%
mutate(time_elapsed_login = interval(last_online, max(profiles$last_online)) / dhours(1))
data <- select(data, -essay0, -last_online)
# Set seed to 1847
set.seed(1847)
# Divide the data randomly with 80% of the data in the training set and 20% in the test set
n <- nrow(data)
test_idx <- sample.int(n, size = round(0.2 * n))
train <- data[-test_idx, ]
test <- data[test_idx, ]
form <- as.formula(likes_dogs~.)
random_forest<- randomForest(form, data = train, mtry=10, ntree=100,na.action=na.roughfix)
typeof(profiles)
typeof(data)
typeof(as.data.frame(data))
lapply(data, level)
lapply(data, levels)
lapply(data, level, length)
lapply(data, levels, length)
length(data$body_type)
unique(data$body_type)
count(data$body_type)
names(levels(data$body_type))
length(levels(data$body_type))
lapply(lapply(data, levels), length)
(lapply(lapply(data, levels), length)) > 53
data[(lapply(lapply(data, levels), length)) > 53]
data[(lapply(lapply(data, levels), length)) < 53]
View(data)
cats <- c("dislikes cats", "has cats", "likes cats")
positive <- c("likes dogs and dislikes cats", "likes dogs and has cats", "likes dogs and likes cats", "likes dogs")
neutral <- c("has dogs and has cats", "has dogs and likes cats", "has dogs", "has dogs and dislikes cats")
negative <- c("dislikes dogs and likes cats", "dislikes dogs", "dislikes dogs and dislikes cats", "dislikes dogs and has cats")
# Remove NA's and info about cats
data <- filter(profiles, !(pets %in% cats), !is.na(pets))
# Create a 1-0 indicator variable for the likes_dogs
data$likes_dogs <- ifelse(data$pets %in% positive, 1, 0)
data$likes_dogs <- as.factor(data$likes_dogs)
# Create factors
data <- data %>% mutate_if(is.character, as.factor)
data <- data %>%
mutate(time_elapsed_login = interval(last_online, max(profiles$last_online)) / dhours(1))
data <- select(data, -essay0, -last_online)
data <- data[(lapply(lapply(data, levels), length)) < 53]
# Set seed to 1847
set.seed(1847)
# Divide the data randomly with 80% of the data in the training set and 20% in the test set
n <- nrow(data)
test_idx <- sample.int(n, size = round(0.2 * n))
train <- data[-test_idx, ]
test <- data[test_idx, ]
form <- as.formula(likes_dogs~.)
random_forest<- randomForest(form, data = train, mtry=10, ntree=100,na.action=na.roughfix)
#pca <- prcomp(train, scale = TRUE)
summary(random_forest)
print(random_forest)
importance(random_forest)
sqrt(19)
form <- as.formula(likes_dogs~.)
random_forest<- randomForest(form, data = train, mtry=5, ntree=500,na.action=na.roughfix)
importance(random_forest)
cats <- c("dislikes cats", "has cats", "likes cats")
positive <- c("likes dogs and dislikes cats", "likes dogs and has cats", "likes dogs and likes cats", "likes dogs")
neutral <- c("has dogs and has cats", "has dogs and likes cats", "has dogs", "has dogs and dislikes cats")
negative <- c("dislikes dogs and likes cats", "dislikes dogs", "dislikes dogs and dislikes cats", "dislikes dogs and has cats")
# Remove NA's and info about cats
data <- filter(profiles, !(pets %in% cats), !is.na(pets))
# Create a 1-0 indicator variable for the likes_dogs
data$likes_dogs <- ifelse(data$pets %in% positive, 1, 0)
data$likes_dogs <- as.factor(data$likes_dogs)
# Create factors
data <- data %>% mutate_if(is.character, as.factor)
data <- data %>%
mutate(time_elapsed_login = interval(last_online, max(profiles$last_online)) / dhours(1))
data <- select(data, -essay0, -last_online, -pets)
data <- data[(lapply(lapply(data, levels), length)) < 53]
# Set seed to 1847
set.seed(1847)
# Divide the data randomly with 80% of the data in the training set and 20% in the test set
n <- nrow(data)
test_idx <- sample.int(n, size = round(0.2 * n))
train <- data[-test_idx, ]
test <- data[test_idx, ]
# Set seed to 1847
set.seed(1847)
# Divide the data randomly with 80% of the data in the training set and 20% in the test set
n <- nrow(data)
test_idx <- sample.int(n, size = round(0.2 * n))
train <- data[-test_idx, ]
test <- data[test_idx, ]
form <- as.formula(likes_dogs~.)
random_forest<- randomForest(form, data = train, mtry=5, ntree=500,na.action=na.roughfix)
importance(random_forest)
sqrt(18)
form <- as.formula(likes_dogs~.)
random_forest<- randomForest(form, data = train, mtry=18, ntree=500,na.action=na.roughfix)
form <- as.formula(likes_dogs~.)
random_forest<- randomForest(form, data = train, mtry=17, ntree=100,na.action=na.roughfix)
importance(random_forest)
form <- as.formula(likes_dogs~.)
random_forest<- randomForest(form, data = train, mtry=5, ntree=500,na.action=na.roughfix)
importance(random_forest)
test_table<-test["likes_dogs"]
test_table$pred<-predict(random_forest, newdata=test, type="class")
sum(diag(confusion_mat)) / sum(confusion_mat)
test_table<-test["likes_dogs"]
test_table$pred<-predict(random_forest, newdata=test, type="class")
confusion_mat<-table(test_table$pred, test_table$likes_dogs)
sum(diag(confusion_mat)) / sum(confusion_mat)
form <- as.formula(likes_dogs~.)
random_forest<- randomForest(form, data = data, mtry=5, ntree=500,na.action=na.roughfix)
importance(random_forest)
View(data)
library(glmnet)
library(tidyverse)
red <- read.csv("C:/Users/pupzo/OneDrive/Documents/Notre Dame/Linear Models/Datasets/winequality-red.csv",header=T,sep=";")
X <- as.matrix(red[,-12])
y <- red[,12]
set.seed(3)
#enter rmse function to compare prediction for ridge regression and ols
rmse <- function(x,y) sqrt(mean((x-y)^2))
#Use the sample and setdiff functions to obtain the indices of observations in each group
n <- dim(red)[1]
train.index <-sample(1:n,round(n/2),replace=F)
test.index <-setdiff(1:n,train.index)
#Now, subset the data into the training and validation parts
X.train <- X[train.index,]; y.train <- y[train.index]
X.test <- X[test.index,]; y.test <- y[test.index]
#Lasso (lasso: alpha = 1)
mod.lasso <- cv.glmnet(X.train, y.train, alpha=1)
# 1
#Obtain test predicted values for lasso
ypred.lasso_0.01 <- cbind(1,X.test)%*%coef(mod.lasso,s=0.01)
# 2
rmse(ypred.lasso_0.01,y.test)
View(ypred.lasso_0.01)
View(ypred.lasso_0.01$x)
ypred.lasso_0.01
red <- read.csv("C:/Users/pupzo/OneDrive/Documents/Notre Dame/Linear Models/Datasets/winequality-red.csv",header=T,sep=";")
X <- as.matrix(red[,-12])
y <- red[,12]
set.seed(3)
#enter rmse function to compare prediction for ridge regression and ols
rmse <- function(x,y) sqrt(mean((x-y)^2))
#Use the sample and setdiff functions to obtain the indices of observations in each group
n <- dim(red)[1]
train.index <-sample(1:n,round(n/2),replace=F)
test.index <-setdiff(1:n,train.index)
#Now, subset the data into the training and validation parts
X.train <- X[train.index,]; y.train <- y[train.index]
X.test <- X[test.index,]; y.test <- y[test.index]
#Lasso (lasso: alpha = 1)
mod.lasso <- cv.glmnet(X.train, y.train, alpha=1)
# 1
coef(mod.lasso,s=0.01)
# 2
ypred.lasso_0.01 <- cbind(1,X.test)%*%coef(mod.lasso,s=0.01)
round(rmse(ypred.lasso_0.01,y.test), 4)
# 3
mod.ols <- lm(y.train~X.train)
ypred.ols <- cbind(1,X.test)%*%coef(mod.ols)
round(rmse(ypred.ols,y.test), 4)
0.6354 > 0.6339
# 5
mod.elastic <- cv.glmnet(X.train, y.train, alpha= 0.5)
# 6
coef(mod.elastic, s=0.01)
red <- read.csv("C:/Users/pupzo/OneDrive/Documents/Notre Dame/Linear Models/Datasets/winequality-red.csv",header=T,sep=";")
X <- as.matrix(red[,-12])
y <- red[,12]
set.seed(3)
#enter rmse function to compare prediction for ridge regression and ols
rmse <- function(x,y) sqrt(mean((x-y)^2))
#Use the sample and setdiff functions to obtain the indices of observations in each group
n <- dim(red)[1]
train.index <-sample(1:n,round(n/2),replace=F)
test.index <-setdiff(1:n,train.index)
#Now, subset the data into the training and validation parts
X.train <- X[train.index,]; y.train <- y[train.index]
X.test <- X[test.index,]; y.test <- y[test.index]
#Lasso (lasso: alpha = 1)
mod.lasso <- cv.glmnet(X.train, y.train, alpha=1)
# 1
coef(mod.lasso,s=0.01) # 2 are "."
# 2
ypred.lasso_0.01 <- cbind(1,X.test)%*%coef(mod.lasso,s=0.01)
round(rmse(ypred.lasso_0.01,y.test), 4) # 0.6339
# 3
mod.ols <- lm(y.train~X.train)
ypred.ols <- cbind(1,X.test)%*%coef(mod.ols)
round(rmse(ypred.ols,y.test), 4) # 0.6354 (slightly larger than)
# 4 lasso with lambda identified in question 1
# 5
mod.elastic <- cv.glmnet(X.train, y.train, alpha= 0.5)
# 6
coef(mod.elastic, s=0.01) # 1 is "."
# 7 lasso and ridge
# 1.1
1 - (((31-1)/(31-3))*(1 - 0.9777))
# 1.1
round(1 - (((31-1)/(31-3))*(1 - 0.9777)), 3)
30/28
(30/28)*(1-0.9888)
1-0.012
#1.6
sqrt(784.0787480 / 0.9908881)
#1.6
round(sqrt(784.0787480 / 0.9908881), 2)
#2
data2 <- mtcars
#2.1
mod2_1 <- lm(mpg ~ cyl + wt, data2)
bc <- MASS::boxcox(mod2_1, plotit = T)
bc$x[which.max(bc$y)]
round(bc$x[which.max(bc$y)] , 2)
#3
data3 <- faraway::cheddar
#3.1
mod3_1 <- lm(taste ~ H2S + I(H2S^2), data3)
summary(mod3_1)
#4
data4 <- faraway::savings
#4.1
mod4_1 <- lm(sr ~ pop15 + ddpi, data = data4)
#4.1
shapiro.test(mod4_1$residuals)
#4.1
round(shapiro.test(mod4_1$residuals), 3)
#4.1
shapiro.test(mod4_1$residuals)
#4.2
lmtest::bptest(mod4_1)
#4.3
lmtest::dwtest(mod4_1)
#4.4
rstandard(mod4_1)
#4.4
which.max(rstandard(mod4_1))
#4.4
std_residuals <- rstandard(mod4_1)
std_residuals[which.max(std_residuals)]
sort(std_residuals, decreasing = T)
round(std_residuals[which.max(std_residuals)], 2)
#4.5
cook <- cooks.distance(mod4_1)
round(cook[which.max(cook)], 2)
sort(cook, decreasing = T)
#4.6
num.df <- 6
den.df <- 50 - 6
f.thresh <- qf(0.5, num.df, den.df)
round(f.thresh, 2)
#4.7
hatv <- hatvalues(mod4_1)
round(max(hatv), 2)
sort(hatv)
round(max(hatv), 2)
sum(hatv)
summary(mod4_1)
#4.6
num.df <- 3
den.df <- 50 - 3
f.thresh <- qf(0.5, num.df, den.df)
round(f.thresh, 2)
#4.7
hatv <- hatvalues(mod4_1)
round(max(hatv), 2)
1-((30/28)*(1-0.9777))
#1.6
round(sqrt(784.0787480 / 0.9908881), 2)
round(bc$x[which.max(bc$y)] , 2)
summary(mod3_1)
#4.1
shapiro.test(mod4_1$residuals)
#4.2
lmtest::bptest(mod4_1)
#4.3
lmtest::dwtest(mod4_1)
round(std_residuals[which.max(std_residuals)], 2)
round(cook[which.max(cook)], 2)
#4.6
num.df <- 3
den.df <- 50 - 3
f.thresh <- qf(0.5, num.df, den.df)
round(f.thresh, 2)
#4.7
hatv <- hatvalues(mod4_1)
round(max(hatv), 2)
sessionInfo()
install.packages('dabestr')
??dabestr
# Performing unpaired (two independent groups) analysis.
unpaired_mean_diff <- dabest(iris, Species, Petal.Width,
idx = c("setosa", "versicolor"),
paired = FALSE)
# Display the results in a user-friendly format.
unpaired_mean_diff
# Produce an estimation plot.
plot(unpaired_mean_diff)
library(dabestr)
# Performing unpaired (two independent groups) analysis.
unpaired_mean_diff <- dabest(iris, Species, Petal.Width,
idx = c("setosa", "versicolor"),
paired = FALSE)
# Display the results in a user-friendly format.
unpaired_mean_diff
# Produce an estimation plot.
plot(unpaired_mean_diff)
install.packages('blogdown')
blogdown::install_hugo()
blogdown:::serve_site()
setwd("~/Website/personal-website")
blogdown:::serve_site()
setwd("~/Website/personal-website/content/post")
blogdown:::serve_site()
